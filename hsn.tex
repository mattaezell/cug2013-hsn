\documentclass[10pt, conference, compsocconf]{IEEEtran}

% Various packages that might be useful
\usepackage[pdftex]{graphicx}
\usepackage{array}
\usepackage[tight,footnotesize]{subfigure}
\usepackage{url}
\hyphenation{op-tical net-works semi-conduc-tor}
\usepackage{color}

\usepackage{tikz}

\usetikzlibrary{arrows}

\begin{document}

\title{Understanding the Impact of Interconnect Failures on System Operation}

\author{\IEEEauthorblockN{Matt Ezell}
\IEEEauthorblockA{High Performance Computing Operations\\
Oak Ridge National Laboratory\\
Oak Ridge, TN\\
ezellma@ornl.gov}
}

\maketitle

\begin{abstract}
Hardware failures are inevitable on large high performance computing systems.
Faults or performance degradations in the high-speed network can reduce the
entire system’s performance. Since the introduction of the Gemini interconnect,
Cray systems have become resilient to many networking faults. These new network
reliability and resiliency features have enabled higher uptimes on Cray systems
by allowing them to continue running with reduced network performance. Oak
Ridge National Laboratory has developed a set of user-level diagnostics that
stresses the high-speed network and searches for components that are not
performing as expected. Nearest-neighbor bandwidth tests check every network
chip and network link in the system. Additionally, performance counters stored
on the network ASIC’s memory mapped registers (MMRs) are used to get a more
full picture of the state of the network. Applications have also been
characterized under various suboptimal network conditions to better understand
what impact network problems have on user codes.
\end{abstract}

\begin{IEEEkeywords}
HPC; Cray; Gemini; Interconnect; HSN; Titan
\end{IEEEkeywords}

\section{Introduction} 

Hardware failures are inevitable on large high performance computing systems.
Problems on a specific compute node can be fatal to an application running on
that node, but these types of faults typically have little to no impact on
other jobs or system services. Faults or performance degradations in the
high-speed network, however, have the potential to reduce the entire system’s
performance.

Since the introduction of the Gemini System Interconnect, Cray high performance
computing systems have become resilient to many types of high-speed networking
faults. Hardware and software monitoring allow the network to transparently
mask out bad lanes within a link, and to reroute around completely failed links
by temporarily pausing network traffic while installing the new routes. These
new network reliability and resiliency features have enabled higher uptimes on
Cray systems by allowing them to continue running with reduced network
performance.

Oak Ridge National Laboratory has developed a set of user-level diagnostics
that stresses the high-speed network and searches for components that are not
performing as expected. Nearest-neighbor bandwidth tests check every network
chip and network link in the system. Unidirectional single-threaded bandwidths
approaching 7 GB/sec have been achieved across healthy links on a quiet system.
Additionally, performance counters stored on the network ASIC’s memory mapped
registers (MMRs) are used to get a more full picture of the state of the
network. Applications have also been characterized under various suboptimal
network conditions to better understand what impact network problems have on
user codes.

\section{The Cray Gemini Network}

Cray XE and XK systems are highly scalable supercomputing platforms based on
the Cray Gemini System Interconnect.  Gemini is a fault-tolerant network
configured in a three dimensional torus.

\subsection{Physical Layout}

At the highest level, XE and XK systems are divided up into cabinets.  Cabinets
are arranged ``left to right'' in rows, and larger systems will have multiple
rows.  Cabinets in the same ``left to right'' position in different rows form
columns.  Each cabinet consists of an L1 cabinet controller, a blower fan,
power conversion electronics, and three chassis.  A chassis (also known as a
cage) holds eight modules (also known as blades).  Modules contain a network
mezzanine that houses two Gemini network chips; each Gemini is shared between
two nodes.  The module printed circuit board layouts are different between
service and compute modules because the service nodes support PCI Express
interface cards.  Furthermore, XE and XK blades differ due to the absence or
presence of a Graphics Processing Unit (GPU).

\begin{figure}
  \centering
  \includegraphics[width=3.0in]{figures/cabinet_diagram.pdf}\\
  \caption{Cabinet Diagram}\label{fig:cab}
\end{figure}

\subsection{3D Torus Topology}

\label{sec:torustopo}
In Gemini's three dimensional torus, each Gemini chip is directly connected to
six of its nearest neighbors: positive and negative X, Y, and Z.  This maps
extremely well to applications that do many nearest-neighbor exchanges, but the
interconnect can be stressed by applications that do all-to-all type
communications.

In general, the X dimension goes along rows, the Y dimension goes along
columns, and the Z dimension goes between the modules in a cabinet.  The
specific cabling rules depend on the size and ``class'' of the system.  For
large systems, X cables connect the 48 Gemini network chips in a cabinet to 48
other Gemini network chips in an X+ cabinet in the same row and 48 other Gemini
network chips in a X- cabinet in the same row.  The length of the X dimension
is equal to the number of columns in a row.  The Y dimension connects the two
Gemini chips that share a mezzanine, and those chips connect to other cabinets
in the same column.  There are 24 Y+ and 24 Y- connections from each cabinet.
The length of the Y dimension is twice the number of rows.  The Z dimension
stays within a cabinet, connecting one Gemini from each module.  There are two
Z-loops per cabinet each with length 24.

Nodes that are close physically are not necessarily close topologically.  Cray
uses a ``folded torus'' to minimize the maximum cable length.  For the X and Y
dimensions, every other cabinet is directly connected together with
``loopback'' cables at the ends to achieve a full torus (see Figure
\ref{fig:cabfold} for an example in the X dimension).  In the Y dimension, the
uppermost chassis connects to the lowermost chassis.

\begin{figure}
  \centering
  \includegraphics[width=3.0in]{figures/cabinets_folded.pdf}\\
  \caption{Example X Cabinet Connections}\label{fig:cabfold}
\end{figure}

\subsection{The Gemini Network Chip}

Many low-level details about Gemini are available in a paper titled ``The
Gemini System Interconnect \cite{hoti}'' that appeared in the 2010 IEEE
Symposium on High Performance Interconnects.

The Gemini ASIC is a 48-port router than provides two network interface
controllers.  The Gemini router is built using a 6x8 array of identical
``tiles.''  Eight tiles referred to as ``ptiles'' are dedicated to the network
interface controllers, and the remaining 40 ``ntiles'' are dedicated to
external connections.  Figure \ref{fig:tiles} shows the tile assignment; being
able to map link numbers to physical locations and dimensions can help in
understanding performance concerns.

\begin{figure}[h]
  \centering
  \includegraphics[width=3.0in]{figures/gemini_tiles.pdf}\\
  \caption{Gemini Tile Layout}\label{fig:tiles}
\end{figure}

The Gemini chip also implements hundreds of performance counters that are
accessible through memory-mapped registers (MMRs).  The network resiliency
software uses the MMRs to control network performance and identify problems,
but they are also accessible to users (most commonly through CrayPat).

\subsection{Network Links}

Each Gemini chip supports two nodes, and those nodes share a topology
coordinate.  Between Gemini chips, there are many ``lanes'' that work in
concert to transfer data.  Three lanes comprise a ``link'' that maps to a
single network tile.  Four links connect in each of the positive and negative
directions in the Y dimension.  For X and Z, there are eight links in each
direction.

\subsection{Fault Tolerence}

The Gemini network was designed to be tolerant of network faults.  Both
hardware and software support allow the system to withstand many different
types of failures without requiring a system reboot.  When a lane fails, the
network is automatically able to run in a degraded mode.  The lane with an
abnormal number of errors is deactivated and the Gemini balances the traffic
over the other two lanes within the channel.  For a configurable period of
time, the Gemini will attempt to re-initialize the failed lane to restore full
bandwidth.  An example of a degraded lane is shown in Figure
\ref{fig:lanedegrade}. 

\begin{figure*}[ht]
  \begin{verbatim}
130325 15:51:00  c0-0c1s0g0l27     c0-0c1s1g0l17       1 Mode Exchanges                       
130325 15:51:00  c0-0c1s0g0l27     c0-0c1s1g0l17       1 RX lanemask=3                        
130325 15:51:00  c0-0c1s0g0l27       ***ERROR*** Gemini LCB lane(s) reinit failed
130325 15:52:00  c0-0c1s1g0l17     c0-0c1s0g0l27       1 TX lanemask=3                       
  \end{verbatim}
  \caption{Single Lane Degrade Seen in Netwatch File}\label{fig:lanedegrade}
\end{figure*}

The lanemask value is a three-bit number corresponding to the three lanes in a
link.  A lanemask of 7 indicates that all lanes are working properly.  A
lanemask of 3, 5, and 6 indicate that a single lane has failed.  A lanemask of
1, 2, or 4 indicates that two lanes have failed.

If an entire link becomes unavailable, the system must take more invasive
action to allow communication to continue.  The Cray Network Link Recovery
Daemon (\emph{nlrd}) on the System Management Workstation (SMW) detects failures and
responds appropriately.  Network traffic is quiesced and new routing tables
(excluding the failed links) are computed and asserted to each Gemini chip.
Then traffic is again allowed to flow using the updated routes.

\subsection{Network Congestion}

Network congestion is a serious problem in any network, especially a 3D torus.
Hot spots in the interconnect can cause variable performance, network timeouts,
or even serious errors.  Not only will the application causing the problem be
affected, but all running processes on the system might be impacted.  Cray
systems employ special hardware and software to avoid congestion.  By reducing
the available injection bandwidth, misbehaving applications are no longer able
to cause congestion.

\subsubsection{Active Throttling}

A network daemon runs on each module controller and regularly samples the MMR
values of the Gemini chips.  When the ratio of network stalls to forwarded
flits (network flow control units) exceeds a configurable value for a
configured period of time, it sends a message to \emph{nlrd} on the SMW indicating
that congestion is occurring.  \emph{nlrd} cooperates with the Application Level
Placement Scheduler (ALPS) to know which applications are running on each node.
\emph{nlrd} sends a message to all the Gemini chips that host the nodes with the
offending application instructing them to throttle.

\subsubsection{Auto-Throttling}

care to talk about the c1 auto-throttles?

\subsubsection{Balanced Injection}

Certain applications or libraries are known to cause congestion due to their
communication pattern.  Improved performance can be obtained by reducing each
node's injection bandwidth to reduce overall network pressure.  The Gemini
supports the ``balanced injection'' feature to reduce credits and outstanding
request buffers.  Cray's MPICH libraries can automatically enable the balanced
injection mode when performing collectives known to cause issues.
Additionally, users can specify a desire to use balanced injection by setting
the \textbf{APRUN\_BALANCED\_INJECTION} environment variable to a positive
integer less than or equal to 100.

\section{The TopoBW Microbenchmark}

Work on better understanding the state of the high speed network began during
acceptance testing on Titan.  Several applications were exhibiting extremely
variable behavior, and it was observed that application placement was an
important factor.  Performance improved significantly by packing applications
into submeshes.  Though it is obvious that optimally placed application have
the ability to perform better than poorly placed ones, the degree to which
performance depended on placement seemed excessive to the author.  A benchmark
application was developed to check the health of the high-speed network to
ensure that there were no undiscovered hardware issues.

\subsection{Program Overview}

The TopoBW microbenchmark was designed to test the bandwidth between
nearest-neighbor partners.  When run on an otherwise idle system, nodes can
accurately measure the speed in which they can send and receive messages from
their neighbors.  The basic outline of the application follows:

\begin{itemize}
	\item Determine its topological coordinates and find its topological neighbors
	\item For each dimension and direction, send and receive from its neighbors
	\item Aggregate the results and calculate minimum, average, and max bandwidths
\end{itemize}

\subsection{Topology Awareness}

The Cray programming environment makes application geometry as well as physical
and topological coordinates available to userspace applications.  The
\emph{PMI} module provides function calls to determine the number of MPMD
applications launched, number of PEs on a given node, and a mapping of ranks to
nodes.  The \emph{RCA} module provides functions to look up a node's physical
characteristics (such as row, column, cage, slot, and node-number), topological
coordinates, and retrieve the maximum value for each dimension.

The application must discover its six nearest neigbhors.  Two nodes share a
Gemini as well as a topological coordinate;  in the code, these are referred to
as ``partners.''  This sharing will affect how much bandwidth is available to a
given node-pair, so partners must communicate if they are participating in a
transfer.

\subsection{MPI Messaging}

Timed MPI routines are used to exchange messages between nodes.  With a given
amount of data sent and a duration for which the transfer lasted, a transfer
speed can be calculated.  To ensure optimal bandwidth utilization, the
asynchronous send/receive MPI calls are used and message receives are
pre-posted.  Five iterations of transfers are run, each using 3,000 concurrent
sends of 4MB each.  The transfer speed is averaged across the runs.

The application begins running tests by cycling through each of the three
dimensions.  First, the ``even'' nodes send in the positive direction to the
``odd'' nodes;  then, the ``odd'' nodes send in the negative direction to the
``even'' ones.  Next, the ``odd'' nodes send in the positive direction to the
``even'' nodes; then, the ``even'' nodes send in the negative direction to the
``odd'' nodes.  In the case of a dimension with an odd length, the first and
last nodes must then send to each other.  Though this algorithm (shown in
Figure \ref{fig:phases}) has more steps than a more straightforward approach,
it avoids having a Gemini sending and receiving at the same time.

\begin{figure}[h]
  \centering
  \begin{tikzpicture}
    \def \phase {1}
    \node[draw] at (-1,-\phase) {Phase \phase};
    \node[draw, circle] at (1,-\phase){$0$};
    \node[draw, circle] at (2,-\phase){$1$};
    \node[draw, circle] at (3,-\phase){$2$};
    \node[draw, circle] at (4,-\phase){$3$};
    \node[draw, circle] at (5,-\phase){$4$};
    \draw[->, >=latex] (1.3,-\phase) to (1.7,-\phase);
    \draw[->, >=latex] (3.3,-\phase) to (3.7,-\phase);

    \def \phase {2}
    \node[draw] at (-1,-\phase) {Phase \phase};
    \node[draw, circle] at (1,-\phase){$0$};
    \node[draw, circle] at (2,-\phase){$1$};
    \node[draw, circle] at (3,-\phase){$2$};
    \node[draw, circle] at (4,-\phase){$3$};
    \node[draw, circle] at (5,-\phase){$4$};
    \draw[<-, >=latex] (1.3,-\phase) to (1.7,-\phase);
    \draw[<-, >=latex] (3.3,-\phase) to (3.7,-\phase);

    \def \phase {3}
    \node[draw] at (-1,-\phase) {Phase \phase};
    \node[draw, circle] at (1,-\phase){$0$};
    \node[draw, circle] at (2,-\phase){$1$};
    \node[draw, circle] at (3,-\phase){$2$};
    \node[draw, circle] at (4,-\phase){$3$};
    \node[draw, circle] at (5,-\phase){$4$};
    \draw[->, >=latex] (2.3,-\phase) to (2.7,-\phase);
    \draw[->, >=latex] (4.3,-\phase) to (4.7,-\phase);

    \def \phase {4}
    \node[draw] at (-1,-\phase) {Phase \phase};
    \node[draw, circle] at (1,-\phase){$0$};
    \node[draw, circle] at (2,-\phase){$1$};
    \node[draw, circle] at (3,-\phase){$2$};
    \node[draw, circle] at (4,-\phase){$3$};
    \node[draw, circle] at (5,-\phase){$4$};
    \draw[<-, >=latex] (2.3,-\phase) to (2.7,-\phase);
    \draw[<-, >=latex] (4.3,-\phase) to (4.7,-\phase);

    \def \phase {5}
    \node[draw] at (-1,-\phase) {Phase \phase};
    \node[draw, circle] at (1,-\phase){$0$};
    \node[draw, circle] at (2,-\phase){$1$};
    \node[draw, circle] at (3,-\phase){$2$};
    \node[draw, circle] at (4,-\phase){$3$};
    \node[draw, circle] at (5,-\phase){$4$};
    \draw[<-, >=latex] (1.28,-4.8 ) arc (160:20:1.84 and .5);

    \def \phase {6}
    \node[draw] at (-1,-\phase) {Phase \phase};
    \node[draw, circle] at (1,-\phase){$0$};
    \node[draw, circle] at (2,-\phase){$1$};
    \node[draw, circle] at (3,-\phase){$2$};
    \node[draw, circle] at (4,-\phase){$3$};
    \node[draw, circle] at (5,-\phase){$4$};
    \draw[->, >=latex] (1.28,-5.8 ) arc (160:20:1.84 and .5);
  \end{tikzpicture}
  \caption{TopoBW Phases}\label{fig:phases}
\end{figure}

\subsection{Reporting}

Using the cabling rules described in Section \ref{sec:torustopo}, the application can identify what type of cable each test was utilizing.

\subsection{Limitations}

\subsection{Results from Titan}

\section{The Latitudes Microbenchmark}

\subsection{MMR Access}
ioctl and gpcd


\section{Real World Applications}

\subsection{S3D: Turbulent Combustion}

\subsubsection{S3D Introduction}



\section{Future Work}

\section{Conclusion}

% use section* for acknowledgement
%\section*{Acknowledgment}

%\textcolor{red}{
%	The authors would like to thank...
%	more thanks here
%}

% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}

\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,hsn}

%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
%\begin{thebibliography}{1}

%\bibitem{Walsh2009}
%John Walsh, Troy Baer, Victor Hazlewood, Junseong Heo, Rick Mohr.
%\newblock{Large Lustre File System Experiences at NICS}.
%\newblock{Cray User Group 2009}.
%\end{thebibliography}


% that's all folks
\end{document}


